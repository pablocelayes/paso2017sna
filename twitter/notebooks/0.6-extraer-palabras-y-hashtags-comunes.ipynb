{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import operator\n",
    "import re\n",
    "import gc\n",
    "import gensim\n",
    "\n",
    "from localsettings import DATA_PATH \n",
    "import json\n",
    "\n",
    "from os.path import join, exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(join(DATA_PATH,'texto_tweets_seguidores_cands')) as f:\n",
    "    tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def preprocess(doc, remove_hashtags=False, remove_accents=False):\n",
    "    pre_doc = doc\n",
    "        \n",
    "    # remover URLs\n",
    "    pre_doc = re.sub(\n",
    "        r\"https?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\",\n",
    "        \" \", pre_doc)\n",
    "    \n",
    "    # minúsculas\n",
    "    pre_doc = pre_doc.lower()\n",
    "\n",
    "    # volar acentos\n",
    "    if remove_accents:\n",
    "        pre_doc = gensim.utils.deaccent(pre_doc)\n",
    "\n",
    "    # remove bullshit\n",
    "    pre_doc = re.sub(r\"\\'|\\\"|\\\\|…|\\/|\\-|\\||\\(|\\)|\\.|\\,|\\!|\\?|\\:|\\;|“|”|’|—\", \" \", pre_doc)\n",
    "    \n",
    "    # contraer vocales\n",
    "#     for v in 'aeiou':\n",
    "#         pre_doc = re.sub(r\"[%s]+\" % v, v, pre_doc)    \n",
    "\n",
    "    # volar menciones\n",
    "    pre_doc = re.sub(r\"\\@\\w+\",\" \", pre_doc)\n",
    "    \n",
    "    # volar hashtags\n",
    "    if remove_hashtags:\n",
    "        pre_doc = re.sub(r\"\\B(\\#[a-zA-Z]+\\b)(?!;)\",\" \", pre_doc)\n",
    "    \n",
    "    # normalizar espacio en blanco\n",
    "    pre_doc = re.sub(r\"\\s+\", \" \", pre_doc)\n",
    "    pre_doc = re.sub(r\"(^\\s)|(\\s$)\", \"\", pre_doc)\n",
    "    \n",
    "    \n",
    "    return pre_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "spanish_tokenizer = load('tokenizers/punkt/spanish.pickle')\n",
    "\n",
    "# stopwords en español\n",
    "spanish_stopwords = stopwords.words('spanish')\n",
    "\n",
    "# spanish stemmer\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "# punctuation to remove\n",
    "non_words = list(punctuation)\n",
    "\n",
    "# we add spanish punctuation\n",
    "non_words.extend(['¿', '¡'])\n",
    "non_words.extend(map(str, range(10)))\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "def trystem(t):\n",
    "    try:\n",
    "        t = stemmer.stem(t)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return t\n",
    "\n",
    "def tokenize(text, stem=False, remove_stopwords=True):\n",
    "    text = preprocess(text, remove_hashtags=True)\n",
    "    result = []\n",
    "    \n",
    "    for sentence in spanish_tokenizer.tokenize(text):\n",
    "        # remover puntuación\n",
    "#         text = ''.join([c for c in sentence if c not in non_words])\n",
    "        \n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [t for t in tokens if t not in spanish_stopwords]\n",
    "\n",
    "        # tokens de al menos 2 letras\n",
    "        tokens = [t for t in tokens if len(t) > 1]\n",
    "            \n",
    "        # stem\n",
    "        if stem:\n",
    "            tokens = [trystem(t) for t in tokens]\n",
    "        \n",
    "        result += tokens\n",
    "        \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 150 palabras más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import codecs\n",
    "\n",
    "for cand, tws in tweets.items():\n",
    "    token_tws = [tokenize(tw) for tw in tws]\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    for tokens in token_tws:\n",
    "        for t in tokens:\n",
    "            counts[t] += 1\n",
    "    \n",
    "    fpath = join(DATA_PATH, 'top_palabras_%s.txt' % cand.lower())\n",
    "    with codecs.open(fpath,'w',encoding='utf8') as f:    \n",
    "        for w, c in sorted(counts.items(), key=lambda x:-x[1])[:150]:\n",
    "            f.write(\"%s: %d\\n\" % (w, c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 150 Hashtags más frecuentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hashtags(t):\n",
    "    return re.findall(r\"\\B(\\#[a-zA-Z]+\\b)(?!;)\", t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cand, tws in tweets.items():\n",
    "    hashtags_cand = [hashtags(preprocess(t)) for t in tws]\n",
    "\n",
    "    counts = defaultdict(int)\n",
    "    for hts in hashtags_cand:\n",
    "        for ht in hts:\n",
    "            counts[ht] += 1\n",
    "\n",
    "    fpath = join(DATA_PATH, 'top_hashtags_%s.txt' % cand.lower())\n",
    "    with codecs.open(fpath,'w',encoding='utf8') as f:    \n",
    "        for w, c in sorted(counts.items(), key=lambda x:-x[1])[:150]:\n",
    "            f.write(\"%s: %d\\n\" % (w, c))            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
